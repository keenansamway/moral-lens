{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84061998",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7dad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Configured API keys: HF_TOKEN, OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, OPENROUTER_API_KEY\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import colorsys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from adjustText import adjust_text\n",
    "\n",
    "from moral_lens.models import load_model_config\n",
    "\n",
    "\n",
    "TAXONOMY_MACRO = {\n",
    "    \"Consequentialism\": [\"MaxDependents\", \"MaxFutureContribution\", \"MaxHope\", \"MaxLifeLength\", \"MaxNumOfLives\", \"SaveTheStrong\", \"MaxInspiration\"],\n",
    "    \"Deontology\": [\"SaveTheUnderprivileged\", \"Egalitarianism\", \"SaveTheVulnerable\", \"AnimalRights\", \"PickRandomly\"],\n",
    "    \"Contractualism\": [\"AppealToLaw\", \"MaxPastContribution\", \"RetributiveJustice\", \"FavorHumans\"],\n",
    "    \"Other\": [\"Other\"],\n",
    "    \"Refusal\": [\"Refusal\"],\n",
    "}\n",
    "\n",
    "TAXONOMY_MICRO = [\n",
    "    micro\n",
    "    for micro_list in TAXONOMY_MACRO.values()\n",
    "    for micro in micro_list\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75635d2",
   "metadata": {},
   "source": [
    "## Query models for results\n",
    "- Can just retrieve from elsewhere if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62a1735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moral_lens.dilemma import DilemmaRunner\n",
    "from moral_lens.judge import JudgeRunner\n",
    "from moral_lens.config import PathConfig\n",
    "from moral_lens.utils import mydisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58dc436",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model_ids = [\n",
    "    # \"openrouter/quasar-alpha\",\n",
    "    # \"openrouter/optimus-alpha\",\n",
    "    # \"gemini-2.0-flash-lite-001\",\n",
    "    \"gemini-2.0-flash-001\",\n",
    "    \"gpt-3.5-turbo-0125\",\n",
    "    # \"gpt-4o-2024-08-06\",\n",
    "    \"gpt-4o-mini-2024-07-18\",\n",
    "    # \"o3-mini-2025-01-31:low\",\n",
    "    \"meta-llama/llama-4-scout\",\n",
    "    # \"meta-llama/llama-4-maverick\",\n",
    "    # \"meta-llama/llama-3.1-8b-instruct\",\n",
    "    # \"meta-llama/llama-3.3-70b-instruct\",\n",
    "    # \"deepseek/deepseek-chat-v3-0324\",\n",
    "    # \"qwen/qwq-32b\",\n",
    "    # \"qwen/qwen-plus\",\n",
    "    # \"microsoft/phi-4\",\n",
    "]\n",
    "# path_config = PathConfig(results_dir=Path(\"moral_lens/experimental_data/decision_consistency\"))\n",
    "path_config = PathConfig(results_dir=Path(\"moral_lens/experimental_data/intersectional_analysis\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac027c2e",
   "metadata": {},
   "source": [
    "Get 3 samples from each model at temperature=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0c011a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gemini model gemini-2.0-flash-001 loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received: 100%|##########| 8/8 [00:01<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gemini-2.0-flash-001_s1.csv.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gemini-2.0-flash-001_s1.csv.\n",
      "[INFO] Gemini model gemini-2.0-flash-001 loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received: 100%|##########| 8/8 [00:01<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gemini-2.0-flash-001_s2.csv.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gemini-2.0-flash-001_s2.csv.\n",
      "[INFO] Gemini model gemini-2.0-flash-001 loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received: 100%|##########| 8/8 [00:00<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gemini-2.0-flash-001_s3.csv.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gemini-2.0-flash-001_s3.csv.\n",
      "[INFO] OpenAI model gpt-3.5-turbo-0125 loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received:  88%|########7 | 7/8 [00:10<00:01,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gpt-3.5-turbo-0125_s1.csv.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gpt-3.5-turbo-0125_s1.csv.\n",
      "[INFO] OpenAI model gpt-3.5-turbo-0125 loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received:  88%|########7 | 7/8 [00:10<00:01,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gpt-3.5-turbo-0125_s2.csv.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gpt-3.5-turbo-0125_s2.csv.\n",
      "[INFO] OpenAI model gpt-3.5-turbo-0125 loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received:  88%|########7 | 7/8 [00:08<00:01,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gpt-3.5-turbo-0125_s3.csv.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gpt-3.5-turbo-0125_s3.csv.\n",
      "[INFO] OpenAI model gpt-4o-mini-2024-07-18 loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received: 100%|##########| 8/8 [00:05<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gpt-4o-mini-2024-07-18_s1.csv.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gpt-4o-mini-2024-07-18_s1.csv.\n",
      "[INFO] OpenAI model gpt-4o-mini-2024-07-18 loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received: 100%|##########| 8/8 [00:05<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gpt-4o-mini-2024-07-18_s2.csv.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gpt-4o-mini-2024-07-18_s2.csv.\n",
      "[INFO] OpenAI model gpt-4o-mini-2024-07-18 loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received: 100%|##########| 8/8 [00:05<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gpt-4o-mini-2024-07-18_s3.csv.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/intersectional_analysis/responses/gpt-4o-mini-2024-07-18_s3.csv.\n",
      "[INFO] OpenRouter model meta-llama/llama-4-scout loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received: 100%|##########| 8/8 [00:02<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses saved to moral_lens/experimental_data/intersectional_analysis/responses/llama-4-scout_s1.csv.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/intersectional_analysis/responses/llama-4-scout_s1.csv.\n",
      "[INFO] OpenRouter model meta-llama/llama-4-scout loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received: 100%|##########| 8/8 [00:01<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses saved to moral_lens/experimental_data/intersectional_analysis/responses/llama-4-scout_s2.csv.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/intersectional_analysis/responses/llama-4-scout_s2.csv.\n",
      "[INFO] OpenRouter model meta-llama/llama-4-scout loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received: 100%|##########| 8/8 [00:02<00:00,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses saved to moral_lens/experimental_data/intersectional_analysis/responses/llama-4-scout_s3.csv.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/intersectional_analysis/responses/llama-4-scout_s3.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for decision_model_id in decision_model_ids:\n",
    "    for experiment in [\"s1\", \"s2\", \"s3\"]:\n",
    "        dr = DilemmaRunner(\n",
    "            model_id=decision_model_id,\n",
    "            decision_run_name=experiment,\n",
    "            path_config=path_config,\n",
    "            choices_filename=\"choices_new.csv\",\n",
    "            override_decision_temperature=0.7,\n",
    "        )\n",
    "        await dr.run()\n",
    "        dr.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e106a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file already exists at moral_lens/experimental_data/decision_consistency/responses/phi-4_s1.csv. Use `overwrite=True` in .run() to overwrite it.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/decision_consistency/responses/phi-4_s1.csv.\n",
      "Output file already exists at moral_lens/experimental_data/decision_consistency/responses/phi-4_s2.csv. Use `overwrite=True` in .run() to overwrite it.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/decision_consistency/responses/phi-4_s2.csv.\n",
      "Output file already exists at moral_lens/experimental_data/decision_consistency/responses/phi-4_s3.csv. Use `overwrite=True` in .run() to overwrite it.\n",
      "[INFO] Processed responses saved to moral_lens/experimental_data/decision_consistency/responses/phi-4_s3.csv.\n"
     ]
    }
   ],
   "source": [
    "for decision_model_id in decision_model_ids:\n",
    "    for experiment in [\"s1\", \"s2\", \"s3\"]:\n",
    "        dr = DilemmaRunner(\n",
    "            model_id=decision_model_id,\n",
    "            decision_run_name=experiment,\n",
    "            path_config=path_config,\n",
    "            # override_decision_temperature=0.7,\n",
    "        )\n",
    "        # await dr.run()\n",
    "        dr.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fd349",
   "metadata": {},
   "source": [
    "## Handle invalid responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73f47b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file already exists at moral_lens/experimental_data/decision_consistency/responses/llama-4-maverick_s1.csv. Use `overwrite=True` in .run() to overwrite it.\n",
      "[INFO] OpenRouter model meta-llama/llama-4-maverick loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received:  25%|##5       | 1/4 [00:11<00:33, 11.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses updated in moral_lens/experimental_data/decision_consistency/responses/llama-4-maverick_s1.csv.\n",
      "Output file already exists at moral_lens/experimental_data/decision_consistency/responses/llama-4-maverick_s2.csv. Use `overwrite=True` in .run() to overwrite it.\n",
      "[INFO] OpenRouter model meta-llama/llama-4-maverick loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received:   0%|          | 0/3 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses updated in moral_lens/experimental_data/decision_consistency/responses/llama-4-maverick_s2.csv.\n",
      "Output file already exists at moral_lens/experimental_data/decision_consistency/responses/llama-4-maverick_s3.csv. Use `overwrite=True` in .run() to overwrite it.\n",
      "[INFO] OpenRouter model meta-llama/llama-4-maverick loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received:   0%|          | 0/3 [00:12<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Responses updated in moral_lens/experimental_data/decision_consistency/responses/llama-4-maverick_s3.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Rerun if needed\n",
    "for decision_model_id in decision_model_ids:\n",
    "    for experiment in [\"s1\", \"s2\", \"s3\"]:\n",
    "        dr = DilemmaRunner(\n",
    "            model_id=decision_model_id,\n",
    "            decision_run_name=experiment,\n",
    "            path_config=path_config,\n",
    "            override_decision_temperature=0.7,\n",
    "        )\n",
    "        dr.load_data()\n",
    "        rows_to_rerun = dr.data[dr.data.decision.str.len() == 0].index.to_list()\n",
    "        await dr.rerun_for_indices(rows_to_rerun)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moral2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
