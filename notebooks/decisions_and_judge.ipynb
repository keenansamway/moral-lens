{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc787a0c",
   "metadata": {},
   "source": [
    "## Query models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7719ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from moral_lens.dilemma import DilemmaRunner\n",
    "from moral_lens.judge import JudgeRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model_ids = [\n",
    "    # # # RQ1 # # #\n",
    "    # \"gpt-4.1-2025-04-14\", # done 5\n",
    "    # \"gpt-4.1-mini-2025-04-14\", # done 5\n",
    "    # \"gpt-4.1-nano-2025-04-14\", # done 5\n",
    "    # \"gpt-4o-2024-11-20\", # done 5\n",
    "    # \"gpt-4o-2024-08-06\", # done 5\n",
    "    # \"gpt-4o-2024-05-13\", # done 5\n",
    "    # \"gpt-4o-mini-2024-07-18\", # done 5\n",
    "    # \"gpt-3.5-turbo-0125\", # done 5\n",
    "    # \"gpt-3.5-turbo-1106\", # done 5\n",
    "    # \"gpt-3.5-turbo-0613\", # depricated\n",
    "\n",
    "    # \"google/gemini-2.5-flash-preview\",\n",
    "    # \"google/gemini-2.0-flash-001\", # done 5\n",
    "    # \"google/gemini-2.0-flash-lite-001\", # done 5\n",
    "    # \"google/gemini-pro-1.5\", # done 5\n",
    "    # \"google/gemini-flash-1.5\", # done 5\n",
    "    # \"google/gemini-flash-1.5-8b\", # done 5\n",
    "\n",
    "    # \"meta-llama/llama-4-maverick\", # done 5\n",
    "    # \"meta-llama/llama-4-scout\", # done 5\n",
    "    # \"meta-llama/llama-3.3-70b-instruct\", # done 5\n",
    "    # \"meta-llama/llama-3.1-405b-instruct\", # 83% approval rate\n",
    "    # \"meta-llama/llama-3.1-70b-instruct\", # done 5\n",
    "    # \"meta-llama/llama-3.1-8b-instruct\", # done 5\n",
    "    # \"meta-llama/llama-3-70b-instruct\", # done 5\n",
    "    # \"meta-llama/llama-3-8b-instruct\", # done 5\n",
    "    # \"meta-llama/llama-2-70b-chat\", # no valid responses in early stages\n",
    "\n",
    "    # \"anthropic/claude-3.7-sonnet:beta\",\n",
    "    # \"anthropic/claude-3.5-sonnet:beta\",\n",
    "    # \"anthropic/claude-3.5-sonnet-20240620:beta\",\n",
    "    # \"anthropic/claude-3.5-haiku:beta\", # done 5\n",
    "    # \"anthropic/claude-3-sonnet:beta\",\n",
    "    # \"anthropic/claude-3-haiku:beta\", # 79% approval rate\n",
    "\n",
    "    # \"microsoft/phi-4\", # done 5\n",
    "    # \"microsoft/phi-3.5-mini-128k-instruct\",\n",
    "    # \"microsoft/phi-3-medium-128k-instruct\", # 89% approval rate\n",
    "    # \"microsoft/phi-3-mini-128k-instruct\", # 88% approval rate\n",
    "\n",
    "    # \"qwen/qwen-max\", # done 5\n",
    "    # \"qwen/qwen-plus\", # done 5\n",
    "    # \"qwen/qwen-turbo\", # done 5\n",
    "\n",
    "    # \"qwen/qwen-2.5-72b-instruct\", # done 5\n",
    "    # \"qwen/qwen-2.5-7b-instruct\", # done 5\n",
    "    # \"qwen/qwen-2-72b-instruct\", # done 5\n",
    "\n",
    "    # \"qwen/qwen3-32b:nothink\",\n",
    "    # \"qwen/qwen3-32b:think\",\n",
    "\n",
    "    # \"qwen/qwen3-30b-a3b:nothink\",\n",
    "    \"qwen/qwen3-30b-a3b:think\",\n",
    "\n",
    "    # \"deepseek/deepseek-chat\", # done 5\n",
    "    # \"deepseek/deepseek-chat-v3-0324\", # done 5\n",
    "    # \"deepseek/deepseek-prover-v2\",\n",
    "\n",
    "    # \"google/gemma-3-27b-it\", # done 5\n",
    "    # \"google/gemma-3-12b-it\", # done 5\n",
    "    # \"google/gemma-3-4b-it\", # done 5\n",
    "\n",
    "    # \"google/gemma-2-27b-it\", # done 5\n",
    "    # \"google/gemma-2-9b-it\", # done 5\n",
    "\n",
    "    # \"cohere/command-r\", # rate limits\n",
    "    # \"cohere/command-r-plus\", # haven't tried\n",
    "\n",
    "    # \"allenai/olmo-7b-instruct\", # Have not tried a full run yet, but I suspect low approval rate\n",
    "\n",
    "    # \"mistralai/mixtral-8x22b-instruct\", # low approval rate\n",
    "    # \"mistralai/mixtral-8x7b-instruct\", # done 5\n",
    "    # \"mistralai/mistral-7b-instruct-v0.3\", # done 5\n",
    "    # \"mistralai/mistral-7b-instruct-v0.1\", # done 5\n",
    "    # \"mistralai/mistral-nemo\", # done 5\n",
    "    # \"mistralai/mistral-small\", # done 5\n",
    "    # \"mistralai/mistral-large\",\n",
    "    # \"mistralai/mistral-large-2407\",\n",
    "\n",
    "    # \"amazon/nova-micro-v1\",\n",
    "    # \"amazon/nova-lite-v1\",\n",
    "    # \"amazon/nova-pro-v1\",\n",
    "\n",
    "\n",
    "    # # # HuggingFace # # #\n",
    "    # \"google/gemma-3-1b-it\",\n",
    "\n",
    "    # # # RQ2 # # #\n",
    "    # \"OLMo-2-0325-32B-SFT\",\n",
    "    # \"OLMo-2-0325-32B-DPO\",\n",
    "    # \"OLMo-2-0325-32B-Instruct\",\n",
    "\n",
    "    # \"OLMo-2-1124-13B-SFT\",\n",
    "    # \"OLMo-2-1124-13B-DPO\",\n",
    "    # \"OLMo-2-1124-13B-Instruct\",\n",
    "\n",
    "    # \"Llama-3.1-Tulu-3-70B-SFT\",\n",
    "    # \"Llama-3.1-Tulu-3-70B-DPO\",\n",
    "    # \"Llama-3.1-Tulu-3-70B\",\n",
    "\n",
    "    # # # RQ3 # # #\n",
    "    # \"tulu-v2.5-dpo-13b-hh-rlhf-60k\",\n",
    "    # \"tulu-v2.5-dpo-13b-chatbot-arena-2023\",\n",
    "    # \"tulu-v2.5-dpo-13b-stackexchange-60k\",\n",
    "    # \"tulu-v2.5-dpo-13b-nectar-60k\",\n",
    "\n",
    "    # \"tulu-v2.5-ppo-13b-hh-rlhf-60k\",\n",
    "    # \"tulu-v2.5-ppo-13b-chatbot-arena-2023\",\n",
    "    # \"tulu-v2.5-ppo-13b-stackexchange-60k\",\n",
    "    # \"tulu-v2.5-ppo-13b-nectar-60k\",\n",
    "\n",
    "    # \"allenai/tulu-2-13b\",\n",
    "\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-uf-mean\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-helpsteer\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-shp2\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-stackexchange\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-uf-overall\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-capybara\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-prm-phase-2\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-hh-rlhf\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-nectar\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-chatbot-arena-2023\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-chatbot-arena-2024\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-alpacafarm-human-pref\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-alpacafarm-gpt4-pref\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-argilla-orca-pairs\",\n",
    "\n",
    "    # # RQ4 # # #\n",
    "    # \"qwen/qwq-32b\", # done 5\n",
    "\n",
    "    # \"deepseek/deepseek-r1\",\n",
    "    # \"deepseek/deepseek-r1-distill-qwen-1.5b\",\n",
    "    # \"deepseek/deepseek-r1-distill-llama-8b\", # done 5\n",
    "    # \"deepseek/deepseek-r1-distill-llama-70b\", # done 5\n",
    "\n",
    "    # \"microsoft/phi-4-reasoning-plus\",\n",
    "\n",
    "    # \"x-ai/grok-3-mini-beta\", # done 5\n",
    "]\n",
    "judge_model_ids = [\n",
    "    # \"gpt-4.1-mini-2025-04-14\",\n",
    "    # \"gpt-4o-2024-08-06\",\n",
    "    \"google/gemini-2.5-flash-preview\",\n",
    "    # \"gemini-2.5-flash-preview-04-17\",\n",
    "]\n",
    "# RESULTS_DIR = \"data/20250507/all_model_runs/\"\n",
    "# RESULTS_DIR = \"data/20250507/test/\"\n",
    "RESULTS_DIR = \"data/20250507/reasoning_model_runs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36934e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = [\n",
    "    \"s1\",\n",
    "    \"s2\",\n",
    "    \"s3\",\n",
    "    \"s4\",\n",
    "    \"s5\",\n",
    "]\n",
    "for decision_model_id in decision_model_ids:\n",
    "    continue_to_next_model = False\n",
    "    for exp in exps:\n",
    "        dr = DilemmaRunner(\n",
    "            model_id=decision_model_id,\n",
    "            decision_run_name=exp,\n",
    "            results_dir=RESULTS_DIR,\n",
    "            # override_decision_temperature=1.0,  # default 0.0\n",
    "            # prompts_template='reasoning_after',\n",
    "            prompts_template='no_reasoning',\n",
    "            choices_filename=\"choices_672.csv\",\n",
    "        )\n",
    "        await dr.run(\n",
    "            # limit=10,\n",
    "            # disable_validation=True,\n",
    "            try_retries=False\n",
    "        )\n",
    "        # dr.process()\n",
    "\n",
    "        # valid_pct = (dr.data.raw_response.str.len() != 0).sum() / len(dr.data)\n",
    "    #     if valid_pct < 0.85:\n",
    "    #         print(f\"Warning: {valid_pct:.2%} of responses for model {decision_model_id} are empty.\")\n",
    "    #         continue_to_next_model = True\n",
    "    #     if continue_to_next_model:\n",
    "    #         continue\n",
    "    # if continue_to_next_model:\n",
    "    #     print(f\"Finished decision model {decision_model_id}.\\n\\n\")\n",
    "    #     continue\n",
    "    print(f\"Finished decision model {decision_model_id}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33362b5",
   "metadata": {},
   "source": [
    "## Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e71ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from moral_lens.dilemma import DilemmaRunner\n",
    "from moral_lens.judge import JudgeRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c025e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model_ids = [\n",
    "    # # # RQ1 # # #\n",
    "    # \"gpt-4.1-2025-04-14\", # done 5\n",
    "    # \"gpt-4.1-mini-2025-04-14\", # done 5\n",
    "    # \"gpt-4.1-nano-2025-04-14\", # done 5\n",
    "    # \"gpt-4o-2024-11-20\", # done 5\n",
    "    # \"gpt-4o-2024-08-06\", # done 5\n",
    "    # \"gpt-4o-2024-05-13\", # done 5\n",
    "    # \"gpt-4o-mini-2024-07-18\", # done 5\n",
    "    # \"gpt-3.5-turbo-0125\", # done 5\n",
    "    # \"gpt-3.5-turbo-1106\", # done 5\n",
    "\n",
    "    # \"google/gemini-2.5-flash-preview\",\n",
    "    # \"google/gemini-2.0-flash-001\", # done 5\n",
    "    # \"google/gemini-2.0-flash-lite-001\", # done 5\n",
    "    # \"google/gemini-pro-1.5\", # done 5\n",
    "    # \"google/gemini-flash-1.5\", # done 5\n",
    "    # \"google/gemini-flash-1.5-8b\", # done 5\n",
    "\n",
    "    # \"meta-llama/llama-4-maverick\", # done 5\n",
    "    # \"meta-llama/llama-4-scout\", # done 5\n",
    "    # \"meta-llama/llama-3.3-70b-instruct\", # done 5\n",
    "    # \"meta-llama/Llama-3.2-3B-Instruct\", # done 5\n",
    "    # \"meta-llama/Llama-3.2-1B-Instruct\", # done 5\n",
    "    # \"meta-llama/llama-3.1-405b-instruct\",\n",
    "    # \"meta-llama/llama-3.1-70b-instruct\", # done 5\n",
    "    # \"meta-llama/llama-3.1-8b-instruct\", # done 5\n",
    "    # \"meta-llama/llama-3-70b-instruct\", # done 5\n",
    "    # \"meta-llama/llama-3-8b-instruct\", # done 5\n",
    "\n",
    "    # \"anthropic/claude-3.7-sonnet:beta\",\n",
    "    # \"anthropic/claude-3.5-sonnet:beta\",\n",
    "    # \"anthropic/claude-3.5-sonnet-20240620:beta\",\n",
    "    # \"anthropic/claude-3.5-haiku:beta\",\n",
    "    # \"anthropic/claude-3-sonnet:beta\",\n",
    "    # \"anthropic/claude-3-haiku:beta\",\n",
    "\n",
    "    # \"microsoft/phi-4\", # done 5\n",
    "    # \"microsoft/phi-3.5-mini-128k-instruct\",\n",
    "    # \"microsoft/phi-3-medium-128k-instruct\", # done 3\n",
    "    # \"microsoft/phi-3-mini-128k-instruct\", # done 3\n",
    "\n",
    "    # \"qwen/qwen-turbo\", # done 5\n",
    "    # \"qwen/qwen-plus\", # done 5\n",
    "    # \"qwen/qwen-max\", # done 5\n",
    "\n",
    "    # \"qwen/qwen-2.5-7b-instruct\", # done 5\n",
    "    # \"qwen/qwen-2.5-72b-instruct\", # done 5\n",
    "\n",
    "    # \"Qwen2.5-1.5B-Instruct\",\n",
    "    # \"Qwen2.5-3B-Instruct\",\n",
    "    # \"Qwen2.5-14B-Instruct\",\n",
    "    # \"Qwen2.5-32B-Instruct\",\n",
    "\n",
    "    # \"Qwen1.5-4B-Chat\",\n",
    "    # \"Qwen1.5-7B-Chat\",\n",
    "    # \"Qwen1.5-14B-Chat\",\n",
    "    # \"Qwen1.5-32B-Chat\",\n",
    "    # \"Qwen1.5-72B-Chat\",\n",
    "\n",
    "    # \"deepseek/deepseek-chat\", # done 5\n",
    "    # \"deepseek/deepseek-chat-v3-0324\", # done 5\n",
    "\n",
    "    # \"google/gemma-3-1b-it\", # done 5\n",
    "    # \"google/gemma-3-4b-it\", # done 5\n",
    "    # \"google/gemma-3-12b-it\", # done 5\n",
    "    # \"google/gemma-3-27b-it\", # done 5\n",
    "\n",
    "    # \"mistralai/mixtral-8x7b-instruct\", # done 5\n",
    "    # \"mistralai/mistral-7b-instruct-v0.3\", # done 5\n",
    "    # \"mistralai/mistral-7b-instruct-v0.1\", # done 5\n",
    "\n",
    "    # # # RQ2 # # #\n",
    "    # \"OLMo-2-0325-32B-SFT\", # done 3\n",
    "    # \"OLMo-2-0325-32B-DPO\", # done 3\n",
    "    # \"OLMo-2-0325-32B-Instruct\",\n",
    "\n",
    "    # \"OLMo-2-1124-13B-SFT\", # done 3\n",
    "    # \"OLMo-2-1124-13B-DPO\", # done 3\n",
    "    # \"OLMo-2-1124-13B-Instruct\",\n",
    "\n",
    "    # \"Llama-3.1-Tulu-3-70B-SFT\", # done 3\n",
    "    # \"Llama-3.1-Tulu-3-70B-DPO\", # done 3\n",
    "    # \"Llama-3.1-Tulu-3-70B\",\n",
    "\n",
    "    # \"amd/Instella-3B-SFT\", # done 3\n",
    "    # \"amd/Instella-3B-Instruct\", # done 3\n",
    "\n",
    "    # # # RQ3 # # #\n",
    "    # \"tulu-v2.5-dpo-13b-hh-rlhf-60k\",\n",
    "    # \"tulu-v2.5-dpo-13b-chatbot-arena-2023\",\n",
    "    # \"tulu-v2.5-dpo-13b-stackexchange-60k\",\n",
    "    # \"tulu-v2.5-dpo-13b-nectar-60k\",\n",
    "\n",
    "    # \"tulu-v2.5-ppo-13b-hh-rlhf-60k\",\n",
    "    # \"tulu-v2.5-ppo-13b-chatbot-arena-2023\",\n",
    "    # \"tulu-v2.5-ppo-13b-stackexchange-60k\",\n",
    "    # \"tulu-v2.5-ppo-13b-nectar-60k\",\n",
    "\n",
    "    # \"allenai/tulu-2-13b\", # done 3\n",
    "\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-uf-mean\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-helpsteer\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-shp2\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-stackexchange\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-uf-overall\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-capybara\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-prm-phase-2\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-hh-rlhf\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-nectar\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-chatbot-arena-2023\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-chatbot-arena-2024\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-alpacafarm-human-pref\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-alpacafarm-gpt4-pref\",\n",
    "    # \"allenai/tulu-v2.5-dpo-13b-argilla-orca-pairs\",\n",
    "\n",
    "    # # RQ4 # # #\n",
    "    # \"qwen/qwq-32b\", # done 5\n",
    "\n",
    "    # \"deepseek/deepseek-r1\",\n",
    "    # \"deepseek/deepseek-r1-distill-qwen-1.5b\",\n",
    "    # \"deepseek/deepseek-r1-distill-llama-8b\",\n",
    "    # \"deepseek/deepseek-r1-distill-llama-70b\", # done 5\n",
    "\n",
    "    # \"microsoft/phi-4-reasoning-plus\",\n",
    "\n",
    "    # \"x-ai/grok-3-mini-beta\", # done 5\n",
    "]\n",
    "judge_model_ids = [\n",
    "    # \"gpt-4.1-mini-2025-04-14\",\n",
    "    # \"gpt-4o-2024-08-06\",\n",
    "    \"google/gemini-2.5-flash-preview\",\n",
    "    # \"gemini-2.5-flash-preview-04-17\",\n",
    "]\n",
    "RESULTS_DIR = \"data/20250507/all_model_runs/\"\n",
    "# RESULTS_DIR = \"data/20250507/dpo_model_runs/\"\n",
    "# RESULTS_DIR = \"data/20250507/preference_runs/\"\n",
    "# RESULTS_DIR = \"data/20250507/reasoning_model_runs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeec6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = [\n",
    "    \"s1\",\n",
    "    \"s2\",\n",
    "    \"s3\",\n",
    "    \"s4\",\n",
    "    \"s5\",\n",
    "]\n",
    "for decision_model_id in decision_model_ids:\n",
    "    for exp in exps:\n",
    "        jr = JudgeRunner(\n",
    "            decision_model_id=decision_model_id,\n",
    "            decision_run_name=exp,\n",
    "            judge_model_id=judge_model_ids[0],\n",
    "            judge_run_name=exp,\n",
    "            results_dir=RESULTS_DIR,\n",
    "            judge_cot=True,\n",
    "            override_judge_temperature=0.0,\n",
    "        )\n",
    "        await jr.run_rationales()\n",
    "\n",
    "    print(f\"Finished analyzing decision model {decision_model_id}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90123c0e",
   "metadata": {},
   "source": [
    "## final qwen runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ffcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model_ids = [\n",
    "    # \"gpt-4.1-mini-2025-04-14\",\n",
    "    # \"gpt-4o-2024-08-06\",\n",
    "    \"google/gemini-2.5-flash-preview\",\n",
    "    # \"gemini-2.5-flash-preview-04-17\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model_ids = [\"qwen/qwen3-30b-a3b:think\"]\n",
    "RESULTS_DIR = \"data/20250507/reasoning_model_runs/\"\n",
    "exps = [\n",
    "    \"s1\",\n",
    "    \"s2\",\n",
    "    \"s3\",\n",
    "    \"s4\",\n",
    "    \"s5\",\n",
    "]\n",
    "for decision_model_id in decision_model_ids:\n",
    "    continue_to_next_model = False\n",
    "    for exp in exps:\n",
    "        dr = DilemmaRunner(\n",
    "            model_id=decision_model_id,\n",
    "            decision_run_name=exp,\n",
    "            results_dir=RESULTS_DIR,\n",
    "            # override_decision_temperature=1.0,  # default 0.0\n",
    "            # prompts_template='reasoning_after',\n",
    "            prompts_template='no_reasoning',\n",
    "            choices_filename=\"choices_672.csv\",\n",
    "        )\n",
    "        await dr.run(\n",
    "            # limit=10,\n",
    "            # disable_validation=True,\n",
    "            try_retries=False\n",
    "        )\n",
    "    print(f\"Finished decision model {decision_model_id}.\\n\\n\")\n",
    "\n",
    "for decision_model_id in decision_model_ids:\n",
    "    for exp in exps:\n",
    "        jr = JudgeRunner(\n",
    "            decision_model_id=decision_model_id,\n",
    "            decision_run_name=exp,\n",
    "            judge_model_id=judge_model_ids[0],\n",
    "            judge_run_name=exp,\n",
    "            results_dir=RESULTS_DIR,\n",
    "            judge_cot=True,\n",
    "            override_judge_temperature=0.0,\n",
    "        )\n",
    "        await jr.run_rationales()\n",
    "\n",
    "    print(f\"Finished analyzing decision model {decision_model_id}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9950cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model_ids = [\"qwen/qwen3-30b-a3b:nothink\"]\n",
    "RESULTS_DIR = \"data/20250507/all_model_runs/\"\n",
    "exps = [\n",
    "    \"s1\",\n",
    "    \"s2\",\n",
    "    \"s3\",\n",
    "    \"s4\",\n",
    "    \"s5\",\n",
    "]\n",
    "for decision_model_id in decision_model_ids:\n",
    "    continue_to_next_model = False\n",
    "    for exp in exps:\n",
    "        dr = DilemmaRunner(\n",
    "            model_id=decision_model_id,\n",
    "            decision_run_name=exp,\n",
    "            results_dir=RESULTS_DIR,\n",
    "            override_decision_temperature=1.0,  # default 0.0\n",
    "            # prompts_template='reasoning_after',\n",
    "            # prompts_template='no_reasoning',\n",
    "            choices_filename=\"choices_672.csv\",\n",
    "        )\n",
    "        await dr.run(\n",
    "            # limit=10,\n",
    "            # disable_validation=True,\n",
    "            try_retries=False\n",
    "        )\n",
    "    print(f\"Finished decision model {decision_model_id}.\\n\\n\")\n",
    "\n",
    "for decision_model_id in decision_model_ids:\n",
    "    for exp in exps:\n",
    "        jr = JudgeRunner(\n",
    "            decision_model_id=decision_model_id,\n",
    "            decision_run_name=exp,\n",
    "            judge_model_id=judge_model_ids[0],\n",
    "            judge_run_name=exp,\n",
    "            results_dir=RESULTS_DIR,\n",
    "            judge_cot=True,\n",
    "            override_judge_temperature=0.0,\n",
    "        )\n",
    "        await jr.run_rationales()\n",
    "\n",
    "    print(f\"Finished analyzing decision model {decision_model_id}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc3832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model_ids = [\"qwen/qwen3-30b-a3b:nothink\"]\n",
    "RESULTS_DIR = \"data/20250507/reasoning_after_runs/\"\n",
    "exps = [\n",
    "    \"after1\",\n",
    "    \"after2\",\n",
    "    \"after3\",\n",
    "    \"after4\",\n",
    "    \"after5\",\n",
    "]\n",
    "for decision_model_id in decision_model_ids:\n",
    "    continue_to_next_model = False\n",
    "    for exp in exps:\n",
    "        dr = DilemmaRunner(\n",
    "            model_id=decision_model_id,\n",
    "            decision_run_name=exp,\n",
    "            results_dir=RESULTS_DIR,\n",
    "            override_decision_temperature=1.0,  # default 0.0\n",
    "            prompts_template='reasoning_after',\n",
    "            # prompts_template='no_reasoning',\n",
    "            choices_filename=\"choices_672.csv\",\n",
    "        )\n",
    "        await dr.run(\n",
    "            # limit=10,\n",
    "            # disable_validation=True,\n",
    "            try_retries=False\n",
    "        )\n",
    "    print(f\"Finished decision model {decision_model_id}.\\n\\n\")\n",
    "\n",
    "for decision_model_id in decision_model_ids:\n",
    "    for exp in exps:\n",
    "        jr = JudgeRunner(\n",
    "            decision_model_id=decision_model_id,\n",
    "            decision_run_name=exp,\n",
    "            judge_model_id=judge_model_ids[0],\n",
    "            judge_run_name=exp,\n",
    "            results_dir=RESULTS_DIR,\n",
    "            judge_cot=True,\n",
    "            override_judge_temperature=0.0,\n",
    "        )\n",
    "        await jr.run_rationales()\n",
    "\n",
    "    print(f\"Finished analyzing decision model {decision_model_id}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952afd6",
   "metadata": {},
   "source": [
    "## Reasoning before vs after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ccb5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from moral_lens.dilemma import DilemmaRunner\n",
    "from moral_lens.judge import JudgeRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6939bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model_ids = [\n",
    "    # \"gpt-4.1-2025-04-14\",\n",
    "    # \"gpt-4.1-mini-2025-04-14\",\n",
    "    # \"gpt-4.1-nano-2025-04-14\", # even with retries, low approval rate in reasoning after\n",
    "    # \"gpt-4o-2024-11-20\",\n",
    "    # \"gpt-4o-2024-08-06\",\n",
    "    # \"gpt-4o-2024-05-13\",\n",
    "    # \"gpt-4o-mini-2024-07-18\",\n",
    "    # \"gpt-3.5-turbo-0125\",\n",
    "    # \"gpt-3.5-turbo-1106\",\n",
    "\n",
    "    # \"google/gemini-2.5-flash-preview\", # we trigger the thinking mechanism in reasoning first\n",
    "    # \"google/gemini-2.0-flash-001\",\n",
    "    # \"google/gemini-2.0-flash-lite-001\",\n",
    "    # \"google/gemini-pro-1.5\",\n",
    "    # \"google/gemini-flash-1.5\",\n",
    "    # \"google/gemini-flash-1.5-8b\",\n",
    "\n",
    "    # \"meta-llama/llama-4-maverick\",\n",
    "    # \"meta-llama/llama-4-scout\",\n",
    "    # \"meta-llama/llama-3.3-70b-instruct\",\n",
    "    # \"meta-llama/llama-3.1-70b-instruct\",\n",
    "    # \"meta-llama/llama-3.1-8b-instruct\",\n",
    "    # \"meta-llama/llama-3-70b-instruct\",\n",
    "    # \"meta-llama/llama-3-8b-instruct\",\n",
    "\n",
    "    # \"anthropic/claude-3.5-sonnet-20240620:beta\",\n",
    "    # \"anthropic/claude-3.5-haiku:beta\",\n",
    "\n",
    "    # \"qwen/qwen-turbo\",\n",
    "    # \"qwen/qwen-plus\",\n",
    "    # \"qwen/qwen-max\",\n",
    "    # \"qwen/qwen-2.5-72b-instruct\",\n",
    "    # \"qwen/qwen-2.5-7b-instruct\",\n",
    "\n",
    "    # \"qwen/qwen-2-72b-instruct\",\n",
    "    # \"google/gemma-2-27b-it\",\n",
    "    # \"google/gemma-2-9b-it\",\n",
    "\n",
    "    # \"Qwen2.5-1.5B-Instruct\",\n",
    "    # \"Qwen2.5-3B-Instruct\",\n",
    "    # \"Qwen2.5-14B-Instruct\",\n",
    "    # \"Qwen2.5-32B-Instruct\",\n",
    "\n",
    "    # \"Qwen1.5-4B-Chat\",\n",
    "    # \"Qwen1.5-7B-Chat\",\n",
    "    # \"Qwen1.5-14B-Chat\",\n",
    "    # \"Qwen1.5-32B-Chat\",\n",
    "    # \"Qwen1.5-72B-Chat\",\n",
    "\n",
    "    \"qwen/qwen3-32b:nothink\",\n",
    "\n",
    "    # \"deepseek/deepseek-chat\",\n",
    "    # \"deepseek/deepseek-chat-v3-0324\",\n",
    "\n",
    "    # \"microsoft/phi-4\",\n",
    "\n",
    "    # \"google/gemma-3-4b-it\",\n",
    "    # \"google/gemma-3-12b-it\",\n",
    "    # \"google/gemma-3-27b-it\",\n",
    "\n",
    "    # \"mistralai/mixtral-8x7b-instruct\", # low approval rate\n",
    "    # \"mistralai/mistral-7b-instruct-v0.3\",\n",
    "    # \"mistralai/mistral-7b-instruct-v0.1\",\n",
    "    # \"mistralai/mistral-nemo\",\n",
    "    # \"mistralai/mistral-small\",\n",
    "    # \"mistralai/mistral-large\",\n",
    "    # \"mistralai/mistral-large-2407\",\n",
    "\n",
    "    # \"amazon/nova-micro-v1\",\n",
    "    # \"amazon/nova-lite-v1\",\n",
    "    # \"amazon/nova-pro-v1\",\n",
    "\n",
    "\n",
    "    # # # RQ2 # # #\n",
    "    # \"OLMo-2-0325-32B-SFT\",\n",
    "    # \"OLMo-2-0325-32B-DPO\",\n",
    "\n",
    "    # \"OLMo-2-1124-13B-SFT\",\n",
    "    # \"OLMo-2-1124-13B-DPO\",\n",
    "]\n",
    "judge_model_ids = [\n",
    "    # \"gpt-4.1-mini-2025-04-14\",\n",
    "    # \"gpt-4o-2024-08-06\",\n",
    "    \"google/gemini-2.5-flash-preview\",\n",
    "    # \"gemini-2.5-flash-preview-04-17\",\n",
    "]\n",
    "RESULTS_DIR = \"data/20250507/reasoning_after_runs/\"\n",
    "# RESULTS_DIR = \"data/20250507/dpo_after_runs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8871f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = [\n",
    "    \"after1\",\n",
    "    \"after2\",\n",
    "    \"after3\",\n",
    "    \"after4\",\n",
    "    \"after5\",\n",
    "]\n",
    "for decision_model_id in decision_model_ids:\n",
    "    continue_to_next_model = False\n",
    "    for exp in exps:\n",
    "        dr = DilemmaRunner(\n",
    "            model_id=decision_model_id,\n",
    "            decision_run_name=exp,\n",
    "            results_dir=RESULTS_DIR,\n",
    "            override_decision_temperature=1.0,  # default 0.0\n",
    "            prompts_template='reasoning_after',\n",
    "            # prompts_template='no_reasoning',\n",
    "            choices_filename=\"choices_672.csv\",\n",
    "        )\n",
    "        await dr.run(\n",
    "            # limit=10,\n",
    "            # disable_validation=True,\n",
    "            # overwrite=True,\n",
    "            try_retries=False,\n",
    "        )\n",
    "        # dr.process()\n",
    "    print(f\"Finished decision model {decision_model_id}.\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4d664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = [\n",
    "    \"after1\",\n",
    "    \"after2\",\n",
    "    \"after3\",\n",
    "    \"after4\",\n",
    "    \"after5\",\n",
    "]\n",
    "for decision_model_id in decision_model_ids:\n",
    "    for exp in exps:\n",
    "        jr = JudgeRunner(\n",
    "            decision_model_id=decision_model_id,\n",
    "            decision_run_name=exp,\n",
    "            judge_model_id=judge_model_ids[0],\n",
    "            judge_run_name=exp,\n",
    "            results_dir=RESULTS_DIR,\n",
    "            judge_cot=True,\n",
    "            override_judge_temperature=0.0,\n",
    "        )\n",
    "        await jr.run_rationales()\n",
    "\n",
    "    print(f\"Finished analyzing decision model {decision_model_id}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5298c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moral2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
