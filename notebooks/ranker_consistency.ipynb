{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec32625",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "301905db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Configured API keys: HF_TOKEN, OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, OPENROUTER_API_KEY\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import colorsys\n",
    "from pathlib import Path\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import numpy as np\n",
    "from adjustText import adjust_text\n",
    "\n",
    "from moral_lens.models import load_model_config\n",
    "\n",
    "\n",
    "TAXONOMY_MACRO = {\n",
    "    \"Consequentialism\": [\"MaxDependents\", \"MaxFutureContribution\", \"MaxHope\", \"MaxLifeLength\", \"MaxNumOfLives\", \"SaveTheStrong\", \"MaxInspiration\"],\n",
    "    \"Deontology\": [\"SaveTheUnderprivileged\", \"Egalitarianism\", \"SaveTheVulnerable\", \"AnimalRights\", \"PickRandomly\"],\n",
    "    \"Contractualism\": [\"AppealToLaw\", \"MaxPastContribution\", \"RetributiveJustice\", \"FavorHumans\"],\n",
    "    \"Other\": [\"Other\"],\n",
    "    \"Refusal\": [\"Refusal\"],\n",
    "}\n",
    "\n",
    "TAXONOMY_MICRO = [\n",
    "    micro\n",
    "    for micro_list in TAXONOMY_MACRO.values()\n",
    "    for micro in micro_list\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa77132",
   "metadata": {},
   "source": [
    "## Query models for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21b4189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moral_lens.dilemma import DilemmaRunner\n",
    "from moral_lens.ranker import RankerRunner\n",
    "from moral_lens.judge import JudgeRunner\n",
    "from moral_lens.config import PathConfig\n",
    "from moral_lens.utils import mydisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cfd476fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model_ids = [\n",
    "    # \"openrouter/quasar-alpha\",\n",
    "    # \"openrouter/optimus-alpha\",\n",
    "    # \"gemini-2.0-flash-lite-001\",\n",
    "    \"gemini-2.0-flash-001\",\n",
    "    # \"gpt-3.5-turbo-0125\",\n",
    "    # \"gpt-4o-2024-08-06\",\n",
    "    # \"gpt-4o-mini-2024-07-18\",\n",
    "    # \"o3-mini-2025-01-31:low\",\n",
    "    # \"meta-llama/llama-4-scout\",\n",
    "    # \"meta-llama/llama-4-maverick\",\n",
    "    # \"meta-llama/llama-3.1-8b-instruct\",\n",
    "    # \"meta-llama/llama-3.3-70b-instruct\",\n",
    "    # \"deepseek/deepseek-chat-v3-0324\",\n",
    "    # \"qwen/qwq-32b\",\n",
    "    # \"qwen/qwen-plus\",\n",
    "    # \"microsoft/phi-4\",\n",
    "]\n",
    "\n",
    "ranker_model_ids = [\n",
    "    # \"gemini-2.0-flash-001\",\n",
    "    # \"meta-llama/llama-4-scout\",\n",
    "    \"gpt-4o-mini-2024-07-18\",\n",
    "    # \"deepseek/deepseek-chat-v3-0324\",\n",
    "]\n",
    "\n",
    "results_dir = \"data/ranker_consistency\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for decision_model_id in decision_model_ids:\n",
    "#     for experiment in [\"s1\", \"s2\", \"s3\"]:\n",
    "#         dr = DilemmaRunner(\n",
    "#             model_id=decision_model_id,\n",
    "#             decision_run_name=experiment,\n",
    "#             results_dir=results_dir,\n",
    "#             # choices_filename=\"choices_10pct.csv\",\n",
    "#             override_decision_temperature=0.7,\n",
    "#         )\n",
    "#         await dr.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f6f980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] OpenAI model gpt-4o-mini-2024-07-18 loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid responses received: 100%|##########| 1000/1000 [04:57<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ranker output saved to data/ranker_consistency/ranker/gpt-4o-mini-2024-07-18.csv\n"
     ]
    }
   ],
   "source": [
    "rr = RankerRunner(\n",
    "    ranker_model_id=ranker_model_ids[0],\n",
    "    # ranker_run_name=\"s1\",\n",
    "    results_dir=results_dir,\n",
    "    ranker_cot=True,\n",
    "    # override_ranker_temperature=0.7,\n",
    ")\n",
    "await rr.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ranker_model_id in ranker_model_ids:\n",
    "    for experiment in [\"s1\", \"s2\", \"s3\"]:\n",
    "        rr = RankerRunner(\n",
    "            ranker_model_id=ranker_model_id,\n",
    "            ranker_run_name=experiment,\n",
    "            results_dir=results_dir,\n",
    "            ranker_cot=True,\n",
    "            override_ranker_temperature=0.7,\n",
    "        )\n",
    "        await jr.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5712e22",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e09dca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def run_elo_analysis(\n",
    "    input_file,\n",
    "    output_dir=\"results\",\n",
    "    initial_rating=1000,\n",
    "    k_factor=32\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a simple Elo analysis on the ranker output file.\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to the CSV file with ranker results\n",
    "        output_dir: Directory to save results and visualizations\n",
    "        initial_rating: Initial Elo rating for all models\n",
    "        k_factor: K-factor for Elo calculations\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, \"plots\"), exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    print(f\"Loading data from {input_file}...\")\n",
    "    df = pd.read_csv(input_file, keep_default_na=False)\n",
    "\n",
    "    # Initialize ratings\n",
    "    unique_models = set()\n",
    "    for model in df['model_a'].unique():\n",
    "        unique_models.add(model)\n",
    "    for model in df['model_b'].unique():\n",
    "        unique_models.add(model)\n",
    "\n",
    "    ratings = {model: initial_rating for model in unique_models}\n",
    "    print(f\"Found {len(unique_models)} unique models\")\n",
    "\n",
    "    # Initialize category tracking\n",
    "    categories = df['phenomenon_category'].unique()\n",
    "    category_performance = {}\n",
    "    for category in categories:\n",
    "        category_performance[category] = defaultdict(lambda: {'wins': 0, 'losses': 0, 'ties': 0})\n",
    "\n",
    "    # Track match history\n",
    "    match_history = []\n",
    "\n",
    "    # Process each comparison\n",
    "    print(\"Computing Elo ratings...\")\n",
    "    for _, row in df.iterrows():\n",
    "        model_a = row['model_a']\n",
    "        model_b = row['model_b']\n",
    "        decision = row['ranker_decision'].lower()  # Convert to lowercase\n",
    "        phenomenon_category = row['phenomenon_category']\n",
    "\n",
    "        # Skip if invalid decision\n",
    "        if decision not in ['a', 'b', 'tie', 'both']:\n",
    "            continue\n",
    "\n",
    "        # Get current ratings\n",
    "        rating_a = ratings[model_a]\n",
    "        rating_b = ratings[model_b]\n",
    "\n",
    "        # Expected scores\n",
    "        expected_a = 1 / (1 + 10 ** ((rating_b - rating_a) / 400))\n",
    "        expected_b = 1 / (1 + 10 ** ((rating_a - rating_b) / 400))\n",
    "\n",
    "        # Actual scores\n",
    "        if decision == 'a':\n",
    "            actual_a, actual_b = 1.0, 0.0\n",
    "            # Update category performance\n",
    "            category_performance[phenomenon_category][model_a]['wins'] += 1\n",
    "            category_performance[phenomenon_category][model_b]['losses'] += 1\n",
    "        elif decision == 'b':\n",
    "            actual_a, actual_b = 0.0, 1.0\n",
    "            # Update category performance\n",
    "            category_performance[phenomenon_category][model_a]['losses'] += 1\n",
    "            category_performance[phenomenon_category][model_b]['wins'] += 1\n",
    "        else:  # tie or both\n",
    "            actual_a, actual_b = 0.5, 0.5\n",
    "            # Update category performance\n",
    "            category_performance[phenomenon_category][model_a]['ties'] += 1\n",
    "            category_performance[phenomenon_category][model_b]['ties'] += 1\n",
    "\n",
    "        # Calculate new ratings\n",
    "        new_rating_a = rating_a + k_factor * (actual_a - expected_a)\n",
    "        new_rating_b = rating_b + k_factor * (actual_b - expected_b)\n",
    "\n",
    "        # Update ratings\n",
    "        ratings[model_a] = new_rating_a\n",
    "        ratings[model_b] = new_rating_b\n",
    "\n",
    "        # Store match history\n",
    "        match_record = {\n",
    "            'model_a': model_a,\n",
    "            'model_b': model_b,\n",
    "            'rating_a_before': rating_a,\n",
    "            'rating_b_before': rating_b,\n",
    "            'rating_a_after': new_rating_a,\n",
    "            'rating_b_after': new_rating_b,\n",
    "            'decision': decision,\n",
    "            'phenomenon_category': phenomenon_category\n",
    "        }\n",
    "        match_history.append(match_record)\n",
    "\n",
    "    # Prepare results\n",
    "    ratings_df = pd.DataFrame([\n",
    "        {'model': model, 'elo_rating': rating}\n",
    "        for model, rating in ratings.items()\n",
    "    ]).sort_values('elo_rating', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Save ratings\n",
    "    ratings_file = os.path.join(output_dir, 'model_elo_ratings.csv')\n",
    "    ratings_df.to_csv(ratings_file, index=False)\n",
    "    print(f\"Saved model ratings to {ratings_file}\")\n",
    "\n",
    "    # Save match history\n",
    "    history_df = pd.DataFrame(match_history)\n",
    "    history_file = os.path.join(output_dir, 'elo_match_history.csv')\n",
    "    history_df.to_csv(history_file, index=False)\n",
    "    print(f\"Saved match history to {history_file}\")\n",
    "\n",
    "    # Generate category performance data\n",
    "    category_records = []\n",
    "    for category, model_data in category_performance.items():\n",
    "        for model, results in model_data.items():\n",
    "            total_matches = results['wins'] + results['losses'] + results['ties']\n",
    "            if total_matches > 0:\n",
    "                win_rate = (results['wins'] + 0.5 * results['ties']) / total_matches\n",
    "                category_records.append({\n",
    "                    'phenomenon_category': category,\n",
    "                    'model': model,\n",
    "                    'wins': results['wins'],\n",
    "                    'losses': results['losses'],\n",
    "                    'ties': results['ties'],\n",
    "                    'total_matches': total_matches,\n",
    "                    'win_rate': win_rate\n",
    "                })\n",
    "\n",
    "    category_df = pd.DataFrame(category_records)\n",
    "    category_file = os.path.join(output_dir, 'category_performance.csv')\n",
    "    category_df.to_csv(category_file, index=False)\n",
    "    print(f\"Saved category performance to {category_file}\")\n",
    "\n",
    "    # Create basic visualizations\n",
    "    print(\"Generating visualizations...\")\n",
    "\n",
    "    # 1. Bar chart of model ratings\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='model', y='elo_rating', data=ratings_df)\n",
    "    plt.title('Model Elo Ratings')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.axhline(y=initial_rating, color='r', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'plots', 'model_elo_ratings.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Heatmap of category performance\n",
    "    if len(category_df) > 0:\n",
    "        pivot_df = category_df.pivot(index='model', columns='phenomenon_category', values='win_rate')\n",
    "        plt.figure(figsize=(max(12, len(categories) * 1.5), max(8, len(unique_models) * 0.5)))\n",
    "        sns.heatmap(pivot_df, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", vmin=0, vmax=1)\n",
    "        plt.title('Model Performance by Category (Win Rate)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'plots', 'category_performance.png'))\n",
    "        plt.close()\n",
    "\n",
    "    # Print summary of results\n",
    "    print(\"\\nTop Models by Elo Rating:\")\n",
    "    print(ratings_df.head(10).to_string(index=False))\n",
    "\n",
    "    print(\"\\nPhenomenon Category Performance Summary:\")\n",
    "    # For each category, show top 3 models\n",
    "    for category in category_df['phenomenon_category'].unique():\n",
    "        cat_data = category_df[category_df['phenomenon_category'] == category].sort_values('win_rate', ascending=False)\n",
    "        print(f\"\\n{category}:\")\n",
    "        print(cat_data.head(3)[['model', 'win_rate', 'total_matches']].to_string(index=False))\n",
    "\n",
    "    print(f\"\\nAll results and visualizations saved to {output_dir}\")\n",
    "    return ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11315f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_consistency(\n",
    "    ranker_output_file,\n",
    "    results_dir=\"results\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze the consistency of decisions for each individual model rather than model pairs.\n",
    "\n",
    "    Args:\n",
    "        ranker_output_file: Path to the CSV file with ranker results\n",
    "        results_dir: Directory to save results and visualizations\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing model consistency from {ranker_output_file}...\")\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(results_dir, \"plots\"), exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(ranker_output_file)\n",
    "\n",
    "    # Extract key columns for identifying unique scenarios\n",
    "    if 'dilemma_prompt' in df.columns:\n",
    "        scenario_id_col = 'dilemma_prompt'\n",
    "    elif 'scenario_text' in df.columns:\n",
    "        scenario_id_col = 'scenario_text'\n",
    "    else:\n",
    "        # If neither is available, try to use 'id' or another identifier\n",
    "        potential_id_cols = ['id', 'scenario_id', 'dilemma_id']\n",
    "        for col in potential_id_cols:\n",
    "            if col in df.columns:\n",
    "                scenario_id_col = col\n",
    "                break\n",
    "        else:\n",
    "            print(\"Could not find a column to identify unique scenarios. Please specify one.\")\n",
    "            return\n",
    "\n",
    "    print(f\"Using '{scenario_id_col}' to identify unique scenarios\")\n",
    "\n",
    "    # Initialize consistency tracking per model\n",
    "    model_consistency = defaultdict(lambda: {\n",
    "        'wins': 0,\n",
    "        'losses': 0,\n",
    "        'consistent_wins': 0,\n",
    "        'consistent_losses': 0,\n",
    "        'inconsistent': 0,\n",
    "        'total_comparisons': 0\n",
    "    })\n",
    "\n",
    "    # Track position bias\n",
    "    position_bias = {\n",
    "        'model_a_wins': 0,\n",
    "        'model_b_wins': 0,\n",
    "        'ties': 0,\n",
    "        'total': 0\n",
    "    }\n",
    "\n",
    "    # Group scenarios\n",
    "    scenario_groups = df.groupby(scenario_id_col)\n",
    "\n",
    "    # For each scenario\n",
    "    for scenario, group in scenario_groups:\n",
    "        # Track comparisons within this scenario\n",
    "        scenario_comparisons = []\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            model_a = row['model_a']\n",
    "            model_b = row['model_b']\n",
    "            decision = row['ranker_decision'].lower()\n",
    "\n",
    "            # Record the comparison\n",
    "            comparison = {\n",
    "                'model_a': model_a,\n",
    "                'model_b': model_b,\n",
    "                'decision': decision\n",
    "            }\n",
    "            scenario_comparisons.append(comparison)\n",
    "\n",
    "            # Update position bias metrics\n",
    "            position_bias['total'] += 1\n",
    "            if decision == 'a':\n",
    "                position_bias['model_a_wins'] += 1\n",
    "            elif decision == 'b':\n",
    "                position_bias['model_b_wins'] += 1\n",
    "            else:  # tie or both\n",
    "                position_bias['ties'] += 1\n",
    "\n",
    "        # Now check for reflected comparisons within this scenario\n",
    "        for i in range(len(scenario_comparisons)):\n",
    "            comp1 = scenario_comparisons[i]\n",
    "            model_a1 = comp1['model_a']\n",
    "            model_b1 = comp1['model_b']\n",
    "            decision1 = comp1['decision']\n",
    "\n",
    "            # Find any reflected comparison (model order swapped)\n",
    "            reflection_found = False\n",
    "            for j in range(len(scenario_comparisons)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "\n",
    "                comp2 = scenario_comparisons[j]\n",
    "                model_a2 = comp2['model_a']\n",
    "                model_b2 = comp2['model_b']\n",
    "                decision2 = comp2['decision']\n",
    "\n",
    "                # Check if this is a reflection (same models, different order)\n",
    "                if (model_a1 == model_b2 and model_b1 == model_a2):\n",
    "                    reflection_found = True\n",
    "\n",
    "                    # Extract actual winners for comparison\n",
    "                    if decision1 == 'a':\n",
    "                        winner1 = model_a1\n",
    "                    elif decision1 == 'b':\n",
    "                        winner1 = model_b1\n",
    "                    else:\n",
    "                        winner1 = \"tie\"\n",
    "\n",
    "                    if decision2 == 'a':\n",
    "                        winner2 = model_a2\n",
    "                    elif decision2 == 'b':\n",
    "                        winner2 = model_b2\n",
    "                    else:\n",
    "                        winner2 = \"tie\"\n",
    "\n",
    "                    # Check if decisions are consistent\n",
    "                    is_consistent = (winner1 == winner2) or winner1 == \"tie\" or winner2 == \"tie\"\n",
    "\n",
    "                    # Update model consistency data\n",
    "                    for model in [model_a1, model_b1]:\n",
    "                        model_consistency[model]['total_comparisons'] += 1\n",
    "\n",
    "                        # Determine if this model won in the first comparison\n",
    "                        if (model == model_a1 and decision1 == 'a') or (model == model_b1 and decision1 == 'b'):\n",
    "                            model_consistency[model]['wins'] += 1\n",
    "                            if is_consistent:\n",
    "                                model_consistency[model]['consistent_wins'] += 1\n",
    "                            else:\n",
    "                                model_consistency[model]['inconsistent'] += 1\n",
    "                        elif decision1 not in ['tie', 'both']:  # Don't count ties as losses\n",
    "                            model_consistency[model]['losses'] += 1\n",
    "                            if is_consistent:\n",
    "                                model_consistency[model]['consistent_losses'] += 1\n",
    "                            else:\n",
    "                                model_consistency[model]['inconsistent'] += 1\n",
    "\n",
    "                    # We found a reflection, no need to check others\n",
    "                    break\n",
    "\n",
    "    # Prepare summary data\n",
    "    model_summary = []\n",
    "    for model, data in model_consistency.items():\n",
    "        total_decisive = data['wins'] + data['losses']\n",
    "        if total_decisive > 0:\n",
    "            consistency_rate = (data['consistent_wins'] + data['consistent_losses']) / total_decisive\n",
    "            win_rate = data['wins'] / total_decisive if total_decisive > 0 else 0\n",
    "\n",
    "            model_summary.append({\n",
    "                'model': model,\n",
    "                'consistency_rate': consistency_rate,\n",
    "                'win_rate': win_rate,\n",
    "                'wins': data['wins'],\n",
    "                'losses': data['losses'],\n",
    "                'consistent_wins': data['consistent_wins'],\n",
    "                'consistent_losses': data['consistent_losses'],\n",
    "                'inconsistent': data['inconsistent'],\n",
    "                'total_comparisons': data['total_comparisons']\n",
    "            })\n",
    "\n",
    "    if model_summary:\n",
    "        # Create DataFrames\n",
    "        model_df = pd.DataFrame(model_summary).sort_values('consistency_rate', ascending=False)\n",
    "\n",
    "        # Save to CSV\n",
    "        model_file = os.path.join(results_dir, 'model_consistency.csv')\n",
    "        model_df.to_csv(model_file, index=False)\n",
    "        print(f\"Saved model consistency to {model_file}\")\n",
    "\n",
    "        # Create visualizations\n",
    "\n",
    "        # 1. Model Consistency Bar Chart\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        # Sort data for consistent presentation\n",
    "        plot_df = model_df.sort_values('consistency_rate', ascending=False)\n",
    "\n",
    "        # Create the barplot with the sorted data\n",
    "        ax = sns.barplot(x='consistency_rate', y='model', data=plot_df)\n",
    "\n",
    "        # Add value labels to the bars\n",
    "        for i, row in enumerate(plot_df.itertuples()):\n",
    "            ax.text(row.consistency_rate + 0.01, i, f\"{row.consistency_rate:.2f}\",\n",
    "               va='center', fontsize=10)\n",
    "\n",
    "        plt.title('Model Consistency Rate')\n",
    "        plt.xlabel('Consistency Rate')\n",
    "        plt.ylabel('Model')\n",
    "        plt.xlim(0, 1.1)  # Leave room for the text labels\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, 'plots', 'model_consistency.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 2. Win Rate vs Consistency Rate\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = sns.scatterplot(x='consistency_rate', y='win_rate',\n",
    "                                  size='total_comparisons', sizes=(100, 500),\n",
    "                                  alpha=0.7, data=model_df)\n",
    "\n",
    "        # Add model labels to the points\n",
    "        for i, row in model_df.iterrows():\n",
    "            plt.text(row['consistency_rate'] + 0.01, row['win_rate'],\n",
    "                    row['model'], fontsize=9)\n",
    "\n",
    "        plt.title('Model Win Rate vs Consistency Rate')\n",
    "        plt.xlabel('Consistency Rate')\n",
    "        plt.ylabel('Win Rate')\n",
    "        plt.xlim(0, 1.1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, 'plots', 'win_vs_consistency.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # Calculate position bias\n",
    "        model_a_win_rate = position_bias['model_a_wins'] / position_bias['total'] if position_bias['total'] > 0 else 0\n",
    "        model_b_win_rate = position_bias['model_b_wins'] / position_bias['total'] if position_bias['total'] > 0 else 0\n",
    "        tie_rate = position_bias['ties'] / position_bias['total'] if position_bias['total'] > 0 else 0\n",
    "\n",
    "        # 3. Position Bias Pie Chart\n",
    "        labels = ['Model A Wins', 'Model B Wins', 'Ties']\n",
    "        sizes = [model_a_win_rate, model_b_win_rate, tie_rate]\n",
    "        colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Position Bias in Ranker Decisions')\n",
    "        plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, 'plots', 'position_bias.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\nModel Consistency Summary:\")\n",
    "        print(model_df[['model', 'consistency_rate', 'win_rate', 'total_comparisons']].to_string(index=False))\n",
    "\n",
    "        print(\"\\nPosition Bias Analysis:\")\n",
    "        print(f\"  Model A wins: {position_bias['model_a_wins']} ({model_a_win_rate:.2f})\")\n",
    "        print(f\"  Model B wins: {position_bias['model_b_wins']} ({model_b_win_rate:.2f})\")\n",
    "        print(f\"  Ties: {position_bias['ties']} ({tie_rate:.2f})\")\n",
    "\n",
    "        if abs(model_a_win_rate - model_b_win_rate) > 0.1:  # More than 10% difference\n",
    "            print(f\"  POTENTIAL POSITION BIAS DETECTED\")\n",
    "        else:\n",
    "            print(f\"  No significant position bias detected\")\n",
    "\n",
    "        return model_df\n",
    "    else:\n",
    "        print(\"No suitable comparisons found to analyze model consistency.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283acd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74a2f69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ranker_decision\n",
       "b      549\n",
       "a      427\n",
       "tie     24\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.ranker_decision.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc855137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/ranker_consistency/ranker/gemini-2.0-flash-001.csv...\n",
      "Found 16 unique models\n",
      "Computing Elo ratings...\n",
      "Saved model ratings to data/ranker_consistency/ranker/gemini-2.0-flash-001/model_elo_ratings.csv\n",
      "Saved match history to data/ranker_consistency/ranker/gemini-2.0-flash-001/elo_match_history.csv\n",
      "Saved category performance to data/ranker_consistency/ranker/gemini-2.0-flash-001/category_performance.csv\n",
      "Generating visualizations...\n",
      "\n",
      "Top Models by Elo Rating:\n",
      "                 model  elo_rating\n",
      "                 phi-4 1335.992863\n",
      " deepseek-chat-v3-0324 1176.496335\n",
      "      llama-4-maverick 1144.777196\n",
      "gpt-4o-mini-2024-07-18 1123.229275\n",
      "             qwen-plus 1074.817800\n",
      "     gpt-4o-2024-08-06 1059.214993\n",
      "               qwq-32b 1038.815224\n",
      "llama-3.3-70b-instruct 1011.068646\n",
      "         optimus-alpha  999.394741\n",
      " llama-3.1-8b-instruct  990.572250\n",
      "\n",
      "Phenomenon Category Performance Summary:\n",
      "\n",
      "Utilitarianism:\n",
      "                model  win_rate  total_matches\n",
      "                phi-4  0.833333             18\n",
      "     llama-4-maverick  0.700000             10\n",
      "llama-3.1-8b-instruct  0.625000             16\n",
      "\n",
      "SocialValue:\n",
      "                model  win_rate  total_matches\n",
      "                phi-4  0.962963             54\n",
      "deepseek-chat-v3-0324  0.750000             60\n",
      "            qwen-plus  0.645161             62\n",
      "\n",
      "Fitness:\n",
      "            model  win_rate  total_matches\n",
      "          qwq-32b  0.916667             12\n",
      " llama-4-maverick  0.861111             18\n",
      "gpt-4o-2024-08-06  0.708333             12\n",
      "\n",
      "Age:\n",
      "       model  win_rate  total_matches\n",
      "   qwen-plus  0.900000             20\n",
      "       phi-4  0.818182             22\n",
      "quasar-alpha  0.812500             16\n",
      "\n",
      "Gender:\n",
      "                 model  win_rate  total_matches\n",
      "      llama-4-maverick  0.958333             12\n",
      "llama-3.3-70b-instruct  0.875000              8\n",
      "         optimus-alpha  0.875000              4\n",
      "\n",
      "Species:\n",
      "                 model  win_rate  total_matches\n",
      "gpt-4o-mini-2024-07-18       1.0              4\n",
      " llama-3.1-8b-instruct       1.0              4\n",
      "llama-3.3-70b-instruct       1.0              4\n",
      "\n",
      "All results and visualizations saved to data/ranker_consistency/ranker/gemini-2.0-flash-001\n",
      "Analyzing model consistency from data/ranker_consistency/ranker/gemini-2.0-flash-001.csv...\n",
      "Using 'dilemma_prompt' to identify unique scenarios\n",
      "Saved model consistency to data/ranker_consistency/ranker/gemini-2.0-flash-001/model_consistency.csv\n",
      "\n",
      "Model Consistency Summary:\n",
      "                    model  consistency_rate  win_rate  total_comparisons\n",
      "         llama-4-maverick          0.884298  0.719008                124\n",
      "   llama-3.3-70b-instruct          0.838710  0.532258                126\n",
      "gemini-2.0-flash-lite-001          0.836066  0.147541                124\n",
      "     gemini-2.0-flash-001          0.828125  0.242188                128\n",
      "                    phi-4          0.806452  0.854839                124\n",
      "        gpt-4o-2024-08-06          0.789474  0.570175                126\n",
      "       gpt-3.5-turbo-0125          0.779528  0.204724                128\n",
      "   o3-mini-2025-01-31:low          0.764706  0.260504                124\n",
      "                qwen-plus          0.761905  0.611111                126\n",
      "    llama-3.1-8b-instruct          0.725806  0.508065                124\n",
      "                  qwq-32b          0.721311  0.598361                124\n",
      "            llama-4-scout          0.719008  0.495868                124\n",
      "    deepseek-chat-v3-0324          0.712000  0.624000                128\n",
      "            optimus-alpha          0.686957  0.530435                122\n",
      "             quasar-alpha          0.627119  0.601695                124\n",
      "   gpt-4o-mini-2024-07-18          0.606557  0.516393                124\n",
      "\n",
      "Position Bias Analysis:\n",
      "  Model A wins: 427 (0.43)\n",
      "  Model B wins: 549 (0.55)\n",
      "  Ties: 24 (0.02)\n",
      "  POTENTIAL POSITION BIAS DETECTED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>consistency_rate</th>\n",
       "      <th>win_rate</th>\n",
       "      <th>wins</th>\n",
       "      <th>losses</th>\n",
       "      <th>consistent_wins</th>\n",
       "      <th>consistent_losses</th>\n",
       "      <th>inconsistent</th>\n",
       "      <th>total_comparisons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama-4-maverick</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>0.719008</td>\n",
       "      <td>87</td>\n",
       "      <td>34</td>\n",
       "      <td>80</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.532258</td>\n",
       "      <td>66</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>48</td>\n",
       "      <td>20</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemini-2.0-flash-lite-001</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>18</td>\n",
       "      <td>104</td>\n",
       "      <td>8</td>\n",
       "      <td>94</td>\n",
       "      <td>20</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gemini-2.0-flash-001</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.242188</td>\n",
       "      <td>31</td>\n",
       "      <td>97</td>\n",
       "      <td>20</td>\n",
       "      <td>86</td>\n",
       "      <td>22</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phi-4</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>106</td>\n",
       "      <td>18</td>\n",
       "      <td>94</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>65</td>\n",
       "      <td>49</td>\n",
       "      <td>53</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>0.779528</td>\n",
       "      <td>0.204724</td>\n",
       "      <td>26</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>87</td>\n",
       "      <td>28</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>o3-mini-2025-01-31:low</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.260504</td>\n",
       "      <td>31</td>\n",
       "      <td>88</td>\n",
       "      <td>17</td>\n",
       "      <td>74</td>\n",
       "      <td>28</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen-plus</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>77</td>\n",
       "      <td>49</td>\n",
       "      <td>62</td>\n",
       "      <td>34</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama-3.1-8b-instruct</td>\n",
       "      <td>0.725806</td>\n",
       "      <td>0.508065</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>46</td>\n",
       "      <td>44</td>\n",
       "      <td>34</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>qwq-32b</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.598361</td>\n",
       "      <td>73</td>\n",
       "      <td>49</td>\n",
       "      <td>56</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>llama-4-scout</td>\n",
       "      <td>0.719008</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>34</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deepseek-chat-v3-0324</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>78</td>\n",
       "      <td>47</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>optimus-alpha</td>\n",
       "      <td>0.686957</td>\n",
       "      <td>0.530435</td>\n",
       "      <td>61</td>\n",
       "      <td>54</td>\n",
       "      <td>43</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>quasar-alpha</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.601695</td>\n",
       "      <td>71</td>\n",
       "      <td>47</td>\n",
       "      <td>49</td>\n",
       "      <td>25</td>\n",
       "      <td>44</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o-mini-2024-07-18</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.516393</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>39</td>\n",
       "      <td>35</td>\n",
       "      <td>48</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  consistency_rate  win_rate  wins  losses  \\\n",
       "7            llama-4-maverick          0.884298  0.719008    87      34   \n",
       "8      llama-3.3-70b-instruct          0.838710  0.532258    66      58   \n",
       "5   gemini-2.0-flash-lite-001          0.836066  0.147541    18     104   \n",
       "13       gemini-2.0-flash-001          0.828125  0.242188    31      97   \n",
       "12                      phi-4          0.806452  0.854839   106      18   \n",
       "0           gpt-4o-2024-08-06          0.789474  0.570175    65      49   \n",
       "11         gpt-3.5-turbo-0125          0.779528  0.204724    26     101   \n",
       "14     o3-mini-2025-01-31:low          0.764706  0.260504    31      88   \n",
       "4                   qwen-plus          0.761905  0.611111    77      49   \n",
       "6       llama-3.1-8b-instruct          0.725806  0.508065    63      61   \n",
       "9                     qwq-32b          0.721311  0.598361    73      49   \n",
       "15              llama-4-scout          0.719008  0.495868    60      61   \n",
       "3       deepseek-chat-v3-0324          0.712000  0.624000    78      47   \n",
       "1               optimus-alpha          0.686957  0.530435    61      54   \n",
       "10               quasar-alpha          0.627119  0.601695    71      47   \n",
       "2      gpt-4o-mini-2024-07-18          0.606557  0.516393    63      59   \n",
       "\n",
       "    consistent_wins  consistent_losses  inconsistent  total_comparisons  \n",
       "7                80                 27            14                124  \n",
       "8                56                 48            20                126  \n",
       "5                 8                 94            20                124  \n",
       "13               20                 86            22                128  \n",
       "12               94                  6            24                124  \n",
       "0                53                 37            24                126  \n",
       "11               12                 87            28                128  \n",
       "14               17                 74            28                124  \n",
       "4                62                 34            30                126  \n",
       "6                46                 44            34                124  \n",
       "9                56                 32            34                124  \n",
       "15               43                 44            34                124  \n",
       "3                60                 29            36                128  \n",
       "1                43                 36            36                122  \n",
       "10               49                 25            44                124  \n",
       "2                39                 35            48                124  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file = \"data/ranker_consistency/ranker/gemini-2.0-flash-001.csv\" # gpt-4o-mini-2024-07-18\n",
    "# file = \"data/ranker_consistency/ranker/gpt-4o-mini-2024-07-18.csv\"\n",
    "results_dir = \"data/ranker_consistency/ranker/\" + file.split(\"/\")[-1].split(\".csv\")[0]\n",
    "\n",
    "run_elo_analysis(\n",
    "    input_file=file,\n",
    "    initial_rating=1000,\n",
    "    k_factor=32,\n",
    "    output_dir=results_dir,\n",
    "    # filter_categories=args.filter_categories\n",
    ")\n",
    "\n",
    "analyze_model_consistency(\n",
    "    ranker_output_file=file,\n",
    "    results_dir=results_dir,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moral2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
